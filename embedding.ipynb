{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Link: https://www.tensorflow.org/alpha/tutorials/text/word_embeddings\n",
    "\n",
    "- We want a dense representation, hence OHC is bad\n",
    "- We want embedding to have similarity meanings (similar words have similar embeddings), hence random index (eg. frequency index) is bad\n",
    "\n",
    "Embedding layer\n",
    "\n",
    "- Takes (batch, sequence_length) as input; eg 32 sentences, where each sentence is a same-length integer vectors (so use frequency index before that)\n",
    "- It can take variable sequence lengths across batches; this is achieved by using a GlobalAveragePooling1D layer\n",
    "- Output: (batch, sequence_length, embedding_dimensionality), so each word in each sentence is turned into a floating point vector with size embedding_dimensionality\n",
    "\n",
    "Gotchas\n",
    "- numpy has to be 1.16.2\n",
    "\n",
    "Q\n",
    "- So we always need to have labeled data to train embeddings...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000, 32) # (vocabulary size, embedding output size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "vocab_size = 10000\n",
    "imdb = keras.datasets.imdb\n",
    "# labels: 1 for positive; 0 for negative\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
    "print(train_data.shape, train_labels.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0]\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:10])\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print(len(word_index)) # 88584\n",
    "\n",
    "# The first indices are reserved\n",
    "# Todo: should we increase vocab_size by 4 then? no -- see below\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  \n",
    "word_index[\"<UNUSED>\"] = 3 # never used\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# does train_data have these special characters?\n",
    "# it does not have <PAD> and <UNUSED>\n",
    "def contains(i):\n",
    "    for l in train_data:\n",
    "        try:\n",
    "            l.index(i)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return False\n",
    "for i in [0,1,2,3]:\n",
    "    print(contains(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data vocab size: 9998\n",
      "test data vocab size: 9951\n"
     ]
    }
   ],
   "source": [
    "# test that dataset indeed has <= 10000 vocab size\n",
    "# so after adding <PAD>, train data will have vocab size of 9999, hence setting 10000 is safe\n",
    "s = set()\n",
    "for l in train_data:\n",
    "    s = s.union(set(l))\n",
    "print(\"train data vocab size:\", len(s))\n",
    "s = set()\n",
    "for l in test_data:\n",
    "    s = s.union(set(l))\n",
    "print(\"test data vocab size:\", len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of train data 2494\n",
      "max length of test data 2315\n"
     ]
    }
   ],
   "source": [
    "# max length of train_data and test_data\n",
    "print(\"max length of train data\", max(list(map(lambda x: len(x), train_data))))\n",
    "print(\"max length of test data\", max(list(map(lambda x: len(x), test_data))))\n",
    "# tutorial truncates to 500; todo: increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad\n",
    "maxlen = 500\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=maxlen)\n",
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=maxlen), # zero initializer does not work\n",
    "  # average over all words within one sentence, this is simplest way to deal with variable length sentence  \n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'), # without this ~85%, with this ~88%\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting weights:\n",
      "[-0.02960713  0.04779026  0.0245004   0.02629631  0.04664708  0.00642449\n",
      " -0.04180528 -0.00649091  0.02941719 -0.03295922  0.037051    0.03730576\n",
      " -0.02007076 -0.00246441  0.01029216  0.04189518]\n",
      "[ 0.04955132 -0.04416816  0.03132464 -0.01327448 -0.01478958  0.04752263\n",
      " -0.04614146 -0.00361229 -0.0139159  -0.04050081 -0.02818963  0.04721266\n",
      " -0.04109221  0.04445548 -0.03769999 -0.02909354]\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 1s 50us/sample - loss: 0.6922 - accuracy: 0.5297 - val_loss: 0.6907 - val_accuracy: 0.5970\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 1s 46us/sample - loss: 0.6877 - accuracy: 0.6409 - val_loss: 0.6836 - val_accuracy: 0.6938\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 1s 40us/sample - loss: 0.6762 - accuracy: 0.7177 - val_loss: 0.6657 - val_accuracy: 0.7432\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.6511 - accuracy: 0.7604 - val_loss: 0.6334 - val_accuracy: 0.7440\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 1s 43us/sample - loss: 0.6134 - accuracy: 0.7720 - val_loss: 0.5911 - val_accuracy: 0.7866\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 1s 41us/sample - loss: 0.5653 - accuracy: 0.8058 - val_loss: 0.5425 - val_accuracy: 0.8064\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 1s 43us/sample - loss: 0.5137 - accuracy: 0.8319 - val_loss: 0.4956 - val_accuracy: 0.8246\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.4644 - accuracy: 0.8523 - val_loss: 0.4550 - val_accuracy: 0.8414\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.4225 - accuracy: 0.8595 - val_loss: 0.4208 - val_accuracy: 0.8526\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 1s 47us/sample - loss: 0.3849 - accuracy: 0.8753 - val_loss: 0.3929 - val_accuracy: 0.8636\n",
      "Epoch 11/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.3546 - accuracy: 0.8839 - val_loss: 0.3716 - val_accuracy: 0.8674\n",
      "Epoch 12/30\n",
      "20000/20000 [==============================] - 1s 41us/sample - loss: 0.3302 - accuracy: 0.8895 - val_loss: 0.3546 - val_accuracy: 0.8702\n",
      "Epoch 13/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.3086 - accuracy: 0.8963 - val_loss: 0.3407 - val_accuracy: 0.8736\n",
      "Epoch 14/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2912 - accuracy: 0.9000 - val_loss: 0.3300 - val_accuracy: 0.8748\n",
      "Epoch 15/30\n",
      "20000/20000 [==============================] - 1s 40us/sample - loss: 0.2766 - accuracy: 0.9049 - val_loss: 0.3214 - val_accuracy: 0.8788\n",
      "Epoch 16/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2628 - accuracy: 0.9088 - val_loss: 0.3139 - val_accuracy: 0.8796\n",
      "Epoch 17/30\n",
      "20000/20000 [==============================] - 1s 40us/sample - loss: 0.2506 - accuracy: 0.9139 - val_loss: 0.3078 - val_accuracy: 0.8822\n",
      "Epoch 18/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2401 - accuracy: 0.9173 - val_loss: 0.3027 - val_accuracy: 0.8836\n",
      "Epoch 19/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2310 - accuracy: 0.9207 - val_loss: 0.2983 - val_accuracy: 0.8854\n",
      "Epoch 20/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2217 - accuracy: 0.9241 - val_loss: 0.2949 - val_accuracy: 0.8842\n",
      "Epoch 21/30\n",
      "20000/20000 [==============================] - 1s 41us/sample - loss: 0.2134 - accuracy: 0.9262 - val_loss: 0.2918 - val_accuracy: 0.8894\n",
      "Epoch 22/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.2057 - accuracy: 0.9293 - val_loss: 0.2891 - val_accuracy: 0.8904\n",
      "Epoch 23/30\n",
      "20000/20000 [==============================] - 1s 43us/sample - loss: 0.1982 - accuracy: 0.9323 - val_loss: 0.2871 - val_accuracy: 0.8908\n",
      "Epoch 24/30\n",
      "20000/20000 [==============================] - 1s 43us/sample - loss: 0.1915 - accuracy: 0.9344 - val_loss: 0.2861 - val_accuracy: 0.8902\n",
      "Epoch 25/30\n",
      "20000/20000 [==============================] - 1s 45us/sample - loss: 0.1850 - accuracy: 0.9375 - val_loss: 0.2843 - val_accuracy: 0.8910\n",
      "Epoch 26/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.1790 - accuracy: 0.9403 - val_loss: 0.2847 - val_accuracy: 0.8914\n",
      "Epoch 27/30\n",
      "20000/20000 [==============================] - 1s 41us/sample - loss: 0.1735 - accuracy: 0.9421 - val_loss: 0.2851 - val_accuracy: 0.8912\n",
      "Epoch 28/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.1683 - accuracy: 0.9435 - val_loss: 0.2858 - val_accuracy: 0.8904\n",
      "Epoch 29/30\n",
      "20000/20000 [==============================] - 1s 42us/sample - loss: 0.1647 - accuracy: 0.9437 - val_loss: 0.2826 - val_accuracy: 0.8926\n",
      "Epoch 30/30\n",
      "20000/20000 [==============================] - 1s 46us/sample - loss: 0.1584 - accuracy: 0.9465 - val_loss: 0.2829 - val_accuracy: 0.8934\n",
      "weights after training\n",
      "[-0.02960713  0.04779026  0.0245004   0.02629631  0.04664708  0.00642449\n",
      " -0.04180528 -0.00649091  0.02941719 -0.03295922  0.037051    0.03730576\n",
      " -0.02007076 -0.00246441  0.01029216  0.04189518]\n",
      "[ 9.2929415e-02 -1.1102810e-01 -5.9643585e-02  1.4699101e-01\n",
      " -1.0402647e-01  1.9617882e-03 -3.5658181e-02 -1.3560778e-01\n",
      "  8.5113443e-02 -2.4444144e-04 -1.3016009e-01 -4.9966246e-01\n",
      "  8.2448134e-03 -1.1670197e-02  4.8871771e-03 -1.2343030e-01]\n",
      "Has <UNUSED> changed weight?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', # important\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"starting weights:\")\n",
    "unused_before = model.layers[0].get_weights()[0][3]\n",
    "print(unused_before)\n",
    "print(model.layers[0].get_weights()[0][10])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2)\n",
    "\n",
    "print(\"weights after training\")\n",
    "unused_after = model.layers[0].get_weights()[0][3]\n",
    "print(unused_after)\n",
    "print(model.layers[0].get_weights()[0][10])\n",
    "\n",
    "print(\"Has <UNUSED> changed weight?\") # no yay\n",
    "unused_before == unused_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 12us/sample - loss: 0.2937 - accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the trained embeddings\n",
    "e = model.layers[0]\n",
    "embeddings = e.get_weights()[0]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
